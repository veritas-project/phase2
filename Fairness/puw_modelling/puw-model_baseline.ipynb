{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Veritas Fairness Assement - Life Insurance Underwriting Study (sample code)\n",
    "This notebook includes samples of code used in the analysis conducted during the life insurance underwriting case study.\n",
    "\n",
    "It is applicable to insurance underwriting datasets including a life insurance dataset available on\n",
    "[kaggle](https://www.kaggle.com/c/prudential-life-insurance-assessment/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "Written by Sankarshan Mridha (Swiss Re) and Laura Alvarez (Accenture) as an extension to Phase 1 Credit Scoring Use Case code https://github.com/veritas-project/phase1/tree/main/credit_scoring \n",
    "\n",
    "Contact email: Veritas@mas.gov.sg\n",
    "\n",
    "\n",
    "Copyright Â© 2021 Monetary Authority of Singapore\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use\n",
    "this file except in compliance with the License. You may obtain a copy of the\n",
    "License at http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed\n",
    "under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR\n",
    "CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
    "specific language governing permissions and limitations under the Licens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T04:57:07.454530Z",
     "start_time": "2021-06-04T04:57:05.895944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Core Packages\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, brier_score_loss, precision_score,\\\n",
    "recall_score, balanced_accuracy_score\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T09:07:23.195352Z",
     "start_time": "2021-05-27T09:07:22.267496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Our code (autoreload)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.append(\"../utils\")\n",
    "import utility as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T09:07:25.099735Z",
     "start_time": "2021-05-27T09:07:25.068155Z"
    }
   },
   "outputs": [],
   "source": [
    "# High-res plots\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T09:08:57.859020Z",
     "start_time": "2021-05-27T09:08:57.832160Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please modify the following cell to update dataset file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('../dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['BMI_Age'] = all_data['BMI'] * all_data['Ins_Age']\n",
    "\n",
    "med_keyword_columns = all_data.columns[all_data.columns.str.startswith('Medical_Keyword_')]\n",
    "all_data['Med_Keywords_Count'] = all_data[med_keyword_columns].sum(axis=1)\n",
    "\n",
    "mapper = {\n",
    "    'Id': 'Insured ID',\n",
    "    'InsuredInfo_6': 'Gender',\n",
    "    'InsuredInfo_1': 'Race',\n",
    "    'InsuredInfo_4': 'Nationality',\n",
    "    'Family_Hist_1': 'Marital Status',\n",
    "    'InsuredInfo_3': 'Occupation Type',\n",
    "    'Employment_Info_2': 'Occupation Industry',\n",
    "    'Wt': 'Weight',\n",
    "    'Ht': 'Height',\n",
    "    'Medical_History_4': 'Smoker Status',\n",
    "    'Ins_Age': 'Age at Policy Inception',\n",
    "    'Insurance_History_3': 'No. of Life Policies',\n",
    "    'Insurance_History_2': 'No. of Accident Policies',\n",
    "    'Insurance_History_7': 'No. of CI Policies',\n",
    "    'Product_Info_3': 'Duration in force for Medical Plan'\n",
    "}\n",
    "\n",
    "all_data.rename(mapper=mapper, axis=1, inplace=True)\n",
    "# Drop columns we do not have confidence in mapping to\n",
    "drop_columns = ('Medical', 'Family', 'Insurance', 'Product', 'Employment', 'Insurance', 'InsuredInfo')\n",
    "mask = all_data.columns.str.startswith(drop_columns)\n",
    "all_data = all_data.iloc[:,~mask]\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary outcome labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels\n",
    "# 0: {1,2}\n",
    "# 1: {7,8}\n",
    "# -1: the rest\n",
    "all_data['Risk'] = pd.cut(all_data.Response, bins=[0,2,6,8], labels=[0,-1,1])\n",
    "all_data = all_data.astype({\"Risk\": int})\n",
    "all_data.Risk.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T09:07:35.298891Z",
     "start_time": "2021-05-27T09:07:35.263887Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove Response = -1\n",
    "df = all_data.loc[all_data['Risk']!= -1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Code corresponding to section 2.7.3 Step 3: Build and Validate in Veritas Document 4 FEAT Principles Assessment Case Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T10:39:47.461749Z",
     "start_time": "2021-05-27T10:39:47.310693Z"
    }
   },
   "outputs": [],
   "source": [
    "# prepare train & test datasets\n",
    "columns_to_drop = ['Insured ID','Response','Risk', 'Nationality', 'Marital Status'] #droping race at pipeline point  \n",
    "X = df.drop(columns=columns_to_drop)\n",
    "X = X.astype({\"Occupation Industry\": object, \"Occupation Type\": object, \"Smoker Status\": object, \"Gender\": object})\n",
    "y = df['Risk']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "print(f\"X_train.shape: {X_train.shape}, X_test.shape: {X_test.shape}\")\n",
    "print(f\"y_train.shape: {y_train.shape}, y_test.shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, selector(dtype_exclude=[\"object\", \"category\"])),\n",
    "    ('cat', categorical_transformer, selector(dtype_include=[\"object\", \"category\"]))\n",
    "], remainder='passthrough')\n",
    "\n",
    "X_train_transformed = preprocessor.fit_transform(X_train.drop(columns='Race'))# dropping race prior to preprocessor\n",
    "X_test_transformed = preprocessor.transform(X_test.drop(columns='Race'))# dropping race prior to preprocessor\n",
    "print(f\"X_train_transformed.shape: {X_train_transformed.shape}, X_test_transformed.shape: {X_test_transformed.shape}\")\n",
    "\n",
    "print(f\"Class distribution: {np.unique(y_train, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "model1 = LogisticRegression(max_iter=150, random_state=SEED)\n",
    "model1.fit(X_train_transformed, y_train)\n",
    "\n",
    "# predict probabilites\n",
    "y_scores = model1.predict_proba(X_test_transformed)[:,1]\n",
    "\n",
    "# compute AUC\n",
    "print(roc_auc_score(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ROC curve\n",
    "fpr, tpr, th = roc_curve(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal cutoff by max balanced accuracy\n",
    "ba = (tpr + (1 - fpr))/2\n",
    "best_ba = np.max(ba)\n",
    "best_th = th[np.argmax(ba)]\n",
    "best_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute classification metrics by 0.5 cutoff\n",
    "y_pred = np.where(y_scores > best_th, 1, 0)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot balanced accuracy and approval rate vs threshold\n",
    "ba = 0.5*(tpr + 1 - fpr)\n",
    "base_ar = np.mean(y_test.astype(int))\n",
    "ar = base_ar*tpr + (1-base_ar)*fpr\n",
    "plt.plot(th, ba, label='balanced accuracy')\n",
    "plt.plot(th, ar, label='approval rate')\n",
    "plt.plot()\n",
    "plt.scatter(best_th, best_ba, c='r', marker='x', s=100, label='max bal acc')\n",
    "plt.xlabel('Underwriting Threshold')\n",
    "plt.title('Life Insurance Underwriting')\n",
    "plt.xlim((y_pred.min(), y_pred.max()))\n",
    "plt.legend(framealpha=0.3, facecolor='white', fontsize=12, loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Performance\n",
    "Here we quantify the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Code corresponding to section 2.7.3 Step 3: Build and Validate in Veritas Document 4 FEAT Principles Assessment Case Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T05:03:40.507348Z",
     "start_time": "2021-06-04T05:03:40.269175Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n",
    "\n",
    "labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_counts,group_names)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T09:08:30.316274Z",
     "start_time": "2021-05-27T09:08:29.997811Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "def plot_roc(model, X, y):\n",
    "    figure = plt.figure(figsize=(5,5))\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "            label='Random Chance', alpha=.8)\n",
    "    metrics.plot_roc_curve(model, X, y, name='model', alpha=0.3, lw=2, ax=plt.gca())\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=13)\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "plot_roc(model1, X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T06:33:57.395071Z",
     "start_time": "2021-05-28T06:33:57.281001Z"
    }
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "joblib.dump(model1, 'model/model_baseline_lr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T06:33:57.395071Z",
     "start_time": "2021-05-28T06:33:57.281001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bootstrap uncertainty analysis\n",
    "\n",
    "# Metrics based on predictions\n",
    "prediction_metrics = {'True Positive Rate (i.e. sensitivity, or recall)': metrics.recall_score,\n",
    "                      'True Negative Rate (i.e. specificity)': lambda x, y: metrics.recall_score(x, y, pos_label=0),\n",
    "                      'Balanced Accuracy': metrics.balanced_accuracy_score,\n",
    "                      'Positive Predictive Value (precision)': metrics.precision_score}\n",
    "\n",
    "# Metrics based on probabilities\n",
    "probability_metrics = {'Area Under ROC': metrics.roc_auc_score}\n",
    "\n",
    "for name, metric_func in prediction_metrics.items():\n",
    "    print(name, \":\", utils.format_uncertainty(*utils.bootstrap_conf_int(y_test.values, y_pred, metric_func, k=25)))\n",
    "\n",
    "for name, metric_func in probability_metrics.items():\n",
    "    print(name, \":\", utils.format_uncertainty(*utils.bootstrap_conf_int(y_test.values, y_scores, metric_func, k=25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration curve\n",
    "def plot_calibration(bin_true_prob, bin_pred_prob):\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"perfectly calibrated\")\n",
    "    ax1.plot(bin_pred_prob, bin_true_prob, \"s-\",\n",
    "             label=\"model\")\n",
    "\n",
    "    ax2.hist([y_scores[y_test == 1], y_scores[y_test == 0]], label=[\"healthy\", \"risky\"],\n",
    "              histtype='bar', stacked=True)\n",
    "\n",
    "    ax1.set_ylabel(\"Fraction of healthy applicants\", fontsize=14)\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc=\"lower right\", fontsize=12)\n",
    "    ax1.set_title('Model Calibration (reliability curve)', fontsize=16)\n",
    "\n",
    "    ax2.set_xlabel(\"Model Output Probabilities (binned)\", fontsize=14)\n",
    "    ax2.set_ylabel(\"Count\", fontsize=14)\n",
    "    ax2.legend(loc=\"upper left\", ncol=2, fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "bin_true_prob, bin_pred_prob = calibration_curve(y_test, y_scores, n_bins=10)\n",
    "plot_calibration(bin_true_prob, bin_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Isotonic calibration\n",
    "clf_isotonic = CalibratedClassifierCV(model1, cv=3, method='isotonic')\n",
    "clf_isotonic = clf_isotonic.fit(X_train_transformed, y_train)\n",
    "model1_iso = clf_isotonic.predict_proba(X_test_transformed)[:, 1]\n",
    "model1_score = brier_score_loss(y_test, y_scores)\n",
    "clf_isotonic_score = brier_score_loss(y_test, model1_iso)\n",
    "print(model1_score, clf_isotonic_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "puw",
   "language": "python",
   "name": "puw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
